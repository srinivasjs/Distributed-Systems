Celery is a Distributed System to process vast amounts of messages. It is a Task Queue, that also supports Task Scheduling

Task Queue:

A Task Queue distributes work across multiple threads/machines . A Task Queue takes in a unit of work called Task .
Dedicated Worker Processes constantly monitor Task Queues for new work to perform.

Celery communicates via messages, usually using a Broker such as RabbitMQ . A client adds message (a unit of Work), 
which the Broker delivers it to a Worker

A Celery system can contain multiple Brokers and Workers supporting High Availibility and Horizontal Scaling .

Note : Horizontal scaling means that you scale by adding more machines into your pool of resources. (Source WIKI)

Celery can run on a single machine, or multiple machines, or even across Data Centers

Clients and Workers will automatically retry in case of Failures/ connection loss thus enabling HA . Brokers, such
as RabbitMQ support HA in the form of Master/Master or Master/Slave Replication

Source : http://docs.celeryproject.org/en/latest/getting-started/introduction.html

Celery Basics :

Celery library must be instantiated before use, instance being called as an Application (or app in short)
Multiple Celery Applications with different configurations, tasks can co-exist in the same process space

Simple Code Snippet :

from celery import Celery
app = Celery()  << Instantiation of Celery

We could see all supported fuctionalities of this Celery class .
print dir(app)

['AsyncResult', 'Beat', 'GroupResult', 'IS_OSX', 'IS_WINDOWS', 'Pickler', 'ResultSet', 'SYSTEM', 'Task', 'TaskSet', 'TaskSetResult', 'WorkController', 'Worker', '__class__', '__delattr__', '__dict__', '__doc__', '__enter__', '__exit__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_args__', '__reduce_ex__', '__reduce_keys__', '__reduce_v1__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_acquire_connection', '_after_fork', '_autodiscover_tasks', '_config_source', '_finalize_mutex', '_fixups', '_get_backend', '_get_config', '_maybe_close_pool', '_pending', '_pending_defaults', '_pool', '_preconf', '_rgetattr', '_task_from_fun', '_tasks', '_using_v1_reduce', 'accept_magic_kwargs', 'add_defaults', 'amqp', 'amqp_cls', 'annotations', 'autodiscover_tasks', 'autofinalize', 'backend', 'backend_cls', 'broker_connection', 'bugreport', 'builtin_fixups', 'canvas', 'clock', 'close', 'conf', 'config_from_cmdline', 'config_from_envvar', 'config_from_object', 'configured', 'connection', 'connection_or_acquire', 'control', 'control_cls', 'create_task_cls', 'current_task', 'default_connection', 'default_producer', 'either', 'events', 'events_cls', 'finalize', 'finalized', 'fixups', 'loader', 'loader_cls', 'log', 'log_cls', 'mail_admins', 'main', 'now', 'oid', 'on_configure', 'on_init', 'pool', 'prepare_config', 'producer_or_acquire', 'registry_cls', 'select_queues', 'send_task', 'set_as_current', 'set_current', 'set_default', 'setup_security', 'signature', 'start', 'steps', 'subclass_with_self', 'task', 'task_cls', 'tasks', 'timezone', 'user_options', 'worker_main']

When we send a task message in Celery, the message will not contain any Source Code instead the name of the Task
to be executed is sent . Every Worker maintains a mapping of Task names to their actual functions, called Task Registry.

Configuration:

Celery configuration can be manipulated to suit requirements . 

The Application instance that we created before :
app = Celery() 
is lazy, meaning that it will not be evaluated until something is actually needed.

Creating a Celery() instance only creates :
1. A logical clock instance, used by events
2. A Task Registry
3. app.on_init() >> By default, it just initializes doing nothing

Let us look into more details on Tasks ...

Tasks



