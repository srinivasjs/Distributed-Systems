Celery is a Distributed System to process vast amounts of messages. It is a Task Queue, that also supports Task Scheduling

Task Queue:

A Task Queue distributes work across multiple threads/machines . A Task Queue takes in a unit of work called Task .
Dedicated Worker Processes constantly monitor Task Queues for new work to perform.

Celery communicates via messages, usually using a Broker such as RabbitMQ . A client adds message (a unit of Work), 
which the Broker delivers it to a Worker

A Celery system can contain multiple Brokers and Workers supporting High Availibility and Horizontal Scaling .

Note : Horizontal scaling means that you scale by adding more machines into your pool of resources. (Source WIKI)

Celery can run on a single machine, or multiple machines, or even across Data Centers

Clients and Workers will automatically retry in case of Failures/ connection loss thus enabling HA . Brokers, such
as RabbitMQ support HA in the form of Master/Master or Master/Slave Replication

Source : http://docs.celeryproject.org/en/latest/getting-started/introduction.html

Celery Basics :

Celery library must be instantiated before use, instance being called as an Application (or app in short)
Multiple Celery Applications with different configurations, tasks can co-exist in the same process space

Simple Code Snippet :

from celery import Celery
app = Celery()  << Instantiation of Celery

We could see all supported fuctionalities of this Celery class .
print dir(app)

['AsyncResult', 'Beat', 'GroupResult', 'IS_OSX', 'IS_WINDOWS', 'Pickler', 'ResultSet', 'SYSTEM', 'Task', 'TaskSet', 'TaskSetResult', 'WorkController', 'Worker', '__class__', '__delattr__', '__dict__', '__doc__', '__enter__', '__exit__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_args__', '__reduce_ex__', '__reduce_keys__', '__reduce_v1__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_acquire_connection', '_after_fork', '_autodiscover_tasks', '_config_source', '_finalize_mutex', '_fixups', '_get_backend', '_get_config', '_maybe_close_pool', '_pending', '_pending_defaults', '_pool', '_preconf', '_rgetattr', '_task_from_fun', '_tasks', '_using_v1_reduce', 'accept_magic_kwargs', 'add_defaults', 'amqp', 'amqp_cls', 'annotations', 'autodiscover_tasks', 'autofinalize', 'backend', 'backend_cls', 'broker_connection', 'bugreport', 'builtin_fixups', 'canvas', 'clock', 'close', 'conf', 'config_from_cmdline', 'config_from_envvar', 'config_from_object', 'configured', 'connection', 'connection_or_acquire', 'control', 'control_cls', 'create_task_cls', 'current_task', 'default_connection', 'default_producer', 'either', 'events', 'events_cls', 'finalize', 'finalized', 'fixups', 'loader', 'loader_cls', 'log', 'log_cls', 'mail_admins', 'main', 'now', 'oid', 'on_configure', 'on_init', 'pool', 'prepare_config', 'producer_or_acquire', 'registry_cls', 'select_queues', 'send_task', 'set_as_current', 'set_current', 'set_default', 'setup_security', 'signature', 'start', 'steps', 'subclass_with_self', 'task', 'task_cls', 'tasks', 'timezone', 'user_options', 'worker_main']

When we send a task message in Celery, the message will not contain any Source Code instead the name of the Task
to be executed is sent . Every Worker maintains a mapping of Task names to their actual functions, called Task Registry.

Note : In short Celery can be called as an WRAPPER to Message Broker such as RabbitMQ

Configuration:

Celery configuration can be manipulated to suit requirements . 

The Application instance that we created before :
app = Celery() 
is lazy, meaning that it will not be evaluated until something is actually needed.

Creating a Celery() instance only creates :
1. A logical clock instance, used by events
2. A Task Registry
3. app.on_init() >> By default, it just initializes doing nothing

Let us look into more details on Tasks ...

Tasks

Tasks in celery perform 2 roles:
1. What happens when a Task is called (sends a message)
2. What happens when a Worker recieves message

A Task message does not appear until it has been acknowledged by the Worker
A worker can reserve any number of messages while Messages can even be redelivered to another worker
under power failure leading to Worker being killed

Task functions need to be IDEMPOTENT . Just in case to avoid repeated execution of a Task in case it is not idempotent, the
message can be acknowledged before the Execution of a Task.

But still, acks_late config in celery can be set to indicate that Message needs to be acknowledged only after Task returns

When to use acks_late ? (source : http://docs.celeryproject.org/en/latest/faq.html#faq-acks-late-vs-retry)

The acks_late setting would be used when you need the task to be executed again if the worker (for some reason) crashes mid-execution. Itâ€™s important to note that the worker is not known to crash, and if it does it is usually an unrecoverable error that requires human intervention (bug in the worker, or task code).


A simple Task Example :

@app.task(bind=True):
def add(x, y):
    return x+y
    
Note : The bind argument to the task decorator will give access to self (the task type instance).
Task Type Instance : 

app is the Celery instance that we'd created before .

Varoius Task Execution States :

Each and every Task state will have Metadata attached to it :
such as if Task is in STARTED state, the metadata will be such as pid and hostname of the worker process executing the task.

PENDING : Indicates Task is waiting for execution
STARTED : Task has been started
SUCCESS : Task has been successfull
FAILURE  :  Task execution has resulted in Failure
RETRY  :  Task is being retried
REVOKED  :  Task has been revoked

Let's now see how Task System in Celery works :

When tasks are sent, no actual function code is sent with it, just the name of the task to execute. When the worker then receives the message it can look up the name in its task registry to find the execution code.
This means that your workers should always be updated with the same software as the client.

All defined tasks are listed in a registry. The registry contains a list of task names and their task classes. This is the list of tasks built-in to celery. Note that tasks will only be registered when the module they are defined in is imported.

Performance and Strategies

It is better to split a problem defined in task into multiple subtasks rather than defining a single problem in one task. This helps in multiple tasks desired to serve a single problem to be executed in parallel and also does not make a worker from executing a single task for long time making other tasks to remain in wait task queue

Data Locality

The Worker processing a task must be as close as possible to data .
The best way to share data across Workers is to use distributed cache system such as memcached


